{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7a0d2b-d432-4e3c-86d2-f98eb88fae95",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks and Single-Layer Perceptron Model\r\n",
    "\r\n",
    "## 1. Neural Networks\r\n",
    "\r\n",
    "### 1.1 What is a Neural Network?\r\n",
    "\r\n",
    "A neural network is a computational model inspired by the way biological neural networks in the human brain process information. It consists of interconnected layers of nodes (neurons) that work together to learn patterns from data. Neural networks are used in various applications, including image recognition, natural language processing, and more.\r\n",
    "\r\n",
    "### 1.2 Structure of Neural Networks\r\n",
    "\r\n",
    "Neural networks typically consist of three types of layers:\r\n",
    "\r\n",
    "- **Input Layer**: The first layer that receives the input features.\r\n",
    "- **Hidden Layers**: Intermediate layers between input and output layers where computations and learning occur.\r\n",
    "- **Output Layer**: The final layer that produces the prediction or classification result.\r\n",
    "\r\n",
    "### 1.3 How Neural Networks Work\r\n",
    "\r\n",
    "1. **Forward Propagation**: Input data is passed through the network layer by layer, with each layer applying a set of weights and biases to transform the data.\r\n",
    "2. **Activation Functions**: Functions like ReLU, Sigmoid, and Tanh introduce non-linearity to the model, allowing it to learn complex patterns.\r\n",
    "3. **Loss Function**: Measures the difference between the predicted output and the actual target. Common loss functions include Mean Squared Error and Cross Entropy.\r\n",
    "4. **Backpropagation**: A technique for optimizing the model by adjusting weights and biases based on the loss gradient.\r\n",
    "\r\n",
    "## 2. Single-Layer Perceptron (SLP)\r\n",
    "\r\n",
    "### 2.1 What is a Single-Layer Perceptron?\r\n",
    "\r\n",
    "A Single-Layer Perceptron is the simplest form of a neural network consisting of a single layer of weights and biases applied to the input data. It is used for binary classification tasks and can be represented mathematically as:\r\n",
    "\r\n",
    "\\[\r\n",
    "y = \\text{activation}(W \\cdot X + b)\r\n",
    "\\]\r\n",
    "\r\n",
    "where:\r\n",
    "- \\( W \\) is the weight matrix,\r\n",
    "- \\( X \\) is the input vector,\r\n",
    "- \\( b \\) is the bias,\r\n",
    "- \\( \\text{activation} \\) is an activation function like Sigmoid or ReLU.\r\n",
    "\r\n",
    "### 2.2 Why Use NumPy for SLP?\r\n",
    "\r\n",
    "NumPy is a fundamental package for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. When implementing neural networks, NumPy can be used to perform the following:\r\n",
    "\r\n",
    "- **Efficient Computations**: NumPy operations are vectorized and optimized for performance, making them suitable for matrix multiplications and other operations in neural networks.\r\n",
    "- **Simple Implementation**: For educational purposes or small-scale models, NumPy provides a straightforward approach to understand the underlying mechanics of neural networks without the complexity of deep learning frameworks.\r\n",
    "- **Learning Tool**: Using NumPy helps to grasp fundamental concepts before moving on to more advanced libraries like TensorFlow or PyTorch.\r\n",
    "\r\n",
    "## 3. Implementing a Single-Layer Perceptron with NumPy\r\n",
    "\r\n",
    "### 3.1 Example Code\r\n",
    "\r\n",
    "Here is a basic implementation of a Single-Layer Perceptron using NumPy:\r\n",
    "\r\n",
    "```python\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# Define the activation function\r\n",
    "def sigmoid(x):\r\n",
    "    return 1 / (1 + np.exp(-x))\r\n",
    "\r\n",
    "# Define the derivative of the activation function\r\n",
    "def sigmoid_derivative(x):\r\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\r\n",
    "\r\n",
    "# Single-Layer Perceptron class\r\n",
    "class SingleLayerPerceptron:\r\n",
    "    def __init__(self, input_size):\r\n",
    "        # Initialize weights and bias\r\n",
    "        self.weights = np.random.randn(input_size)\r\n",
    "        self.bias = np.random.randn()\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        # Forward pass\r\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\r\n",
    "        return sigmoid(linear_output)\r\n",
    "\r\n",
    "    def train(self, X, y, learning_rate=0.1, epochs=1000):\r\n",
    "        for epoch in range(epochs):\r\n",
    "            # Forward pass\r\n",
    "            predictions = self.predict(X)\r\n",
    "            \r\n",
    "            # Compute the error\r\n",
    "            error = y - predictions\r\n",
    "            \r\n",
    "            # Backward pass (gradient descent)\r\n",
    "            gradient = np.dot(X.T, error * sigmoid_derivative(predictions)) / len(y)\r\n",
    "            self.weights += learning_rate * gradient\r\n",
    "            self.bias += learning_rate * np.sum(error * sigmoid_derivative(predictions)) / len(y)\r\n",
    "\r\n",
    "# Example usage\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    # Example data\r\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\r\n",
    "    y = np.array([0, 1, 1, 0])  # XOR problem\r\n",
    "\r\n",
    "    # Initialize and train the model\r\n",
    "    model = SingleLayerPerceptron(input_size=2)\r\n",
    "    model.train(X, y)\r\n",
    "\r\n",
    "    # Make predictions\r\n",
    "    predictions = model.predict(X)\r\n",
    "    print(\"Predictions:\\n\", predictions)\r\n",
    "  # Make predictions\r\n",
    "    predictions = model.predict(X)\r\n",
    "    print(\"Predictions:\\n\", predictions)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e9d6a7-216d-432e-a27c-51d5fa9876f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b181ac12-7ff2-4dda-8bac-b5efee75e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[0,0],[0,1],[1,0],[1,1]]) #input features\n",
    "y=np.array([0,0,0,10])# OUTPUT labels (AND GATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1726039-94a9-4c4d-98ea-93fcbb967a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=np.random.rand(2)\n",
    "bias=np.random.rand(1)\n",
    "learning_rate=0.1\n",
    "# what is this np.random.rand(2)\n",
    "# What is activation function in a NN\n",
    "# Ans: converts o/p into a format we want  like either 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38bf4c6-3350-40c4-aef5-c454e8c83b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function : Step function\n",
    "def step_function(x):\n",
    "    return 1 if x>0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53b36b5-a6aa-406e-841c-b8f126d81cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the perceptron\n",
    "for epoch in range(10): # check for 100, 1000 epochs and why is the output wrong, is it sufficient for AND gate logic also tell why did the input size in affect a lot\n",
    "    for xi,target in zip(x,y):\n",
    "        #Calculate linear combination\n",
    "        linear_output=np.dot(xi,weights)+bias\n",
    "        #apply activation function\n",
    "        prediction=step_function(linear_output)\n",
    "        #Calculate the error\n",
    "        error=target-prediction\n",
    "        #update weights and bias\n",
    "        weights+= learning_rate*error*xi\n",
    "        bias+=learning_rate*error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d237ccee-98fe-4fd9-ac3c-eb3404e27b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "Final Weights:  [8.53920947 8.15939308]\n",
      "Final Bias:  [6.80997738]\n",
      "Input : [0 0], Ouput: 1\n",
      "Input : [0 1], Ouput: 1\n",
      "Input : [1 0], Ouput: 1\n",
      "Input : [1 1], Ouput: 1\n"
     ]
    }
   ],
   "source": [
    "# Print final weights and biases\n",
    "print(\"RESULTS\")\n",
    "print(\"Final Weights: \",weights)\n",
    "print(\"Final Bias: \",bias)\n",
    "\n",
    "for xi in x:\n",
    "    print(f\"Input : {xi}, Ouput: {step_function(np.dot(xi,weights)+bias)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577b8fd-13af-4650-9025-f3e6076aa433",
   "metadata": {},
   "source": [
    "# Understanding Bias in Neural Networks\r\n",
    "\r\n",
    "## What is Bias in Neural Networks?\r\n",
    "\r\n",
    "### Definition\r\n",
    "\r\n",
    "In the context of neural networks, the **bias** is an additional parameter added to the weighted sum of inputs before applying the activation function. Mathematically, if \\( \\mathbf{X} \\) is the input vector, \\( \\mathbf{W} \\) is the weight vector, and \\( b \\) is the bias, the output of a neuron can be expressed as:\r\n",
    "\r\n",
    "\\[\r\n",
    "y = \\text{activation}( \\mathbf{W} \\cdot \\mathbf{X} + b )\r\n",
    "\\]\r\n",
    "\r\n",
    "where:\r\n",
    "- \\( \\mathbf{W} \\cdot \\mathbf{X} \\) represents the dot product of the weights and inputs,\r\n",
    "- \\( b \\) is the bias term,\r\n",
    "- \\(\\text{activation}\\) is the activation function applied to the weighted sum plus bias.\r\n",
    "\r\n",
    "### Purpose of Bias\r\n",
    "\r\n",
    "- **Shifting the Activation Function**: The bias allows the activation function to be shifted to the left or right, which enables the model to fit the data better. Without the bias, the activation function is always centered at zero, which might not be suitable for all datasets.\r\n",
    "  \r\n",
    "- **Enabling Learning of Complex Patterns**: Bias helps the network learn patterns and relationships in the data that might not be possible if the activation function were always centered at zero. It provides more flexibility to the model by allowing it to fit a broader range of functions.\r\n",
    "\r\n",
    "- **Handling Zero Inputs**: Bias ensures that neurons can activate even if all the input values are zero. This is particularly important because it means the neuron can still produce a non-zero output, which helps in learning and optimization.\r\n",
    "\r\n",
    "### Why is Bias Added Every Time?\r\n",
    "\r\n",
    "- **Generalization**: Bias is added to every neuron to ensure that each neuron can independently adjust its output regardless of the input values. This individual adjustment capability helps in better generalization of the model to various types of data.\r\n",
    "\r\n",
    "- **Improving Model Performance**: Including bias improves the overall performance of the neural network by enabling it to learn a wider variety of functions. This flexibility often results in better fitting of the training data and potentially better generalization to unseen data.\r\n",
    "\r\n",
    "- **Simplicity and Consistency**: Adding a bias term is a standard practice in neural networks and contributes to the simplicity and consistency of the model architecture. It is a well-established technique that helps in optimizing and training neural networks effectively.\r\n",
    "\r\n",
    "### Example\r\n",
    "\r\n",
    "Consider a simple neuron with a single input feature. If we denote the weight as \\( w \\), the input as \\( x \\), and the bias as \\( b \\), the output before applying the activation function can be computed as:\r\n",
    "\r\n",
    "\\[\r\n",
    "z = w \\cdot x + b\r\n",
    "\\]\r\n",
    "\r\n",
    "Here, the bias \\( b \\) adjusts the value of \\( z \\), allowing the activation function to output values that are not restricted by the zero-centered nature of the weighted sum alone.\r\n",
    "\r\n",
    "### Impact on Learning\r\n",
    "\r\n",
    "During the training process of a neural network, both weights and biases are adjusted based on the gradients computed during backpropagation. This adjustment helps minimize the loss function and improves the accuracy of the model. Biases play a critical role in this adjustment by allowing each neuron to adapt its output independently of the input values.\r\n",
    "\r\n",
    "## Summary\r\n",
    "\r\n",
    "- **Bias** is an additional parameter added to the weighted sum of inputs in a neural network.\r\n",
    "- It allows the activation function to be shifted, which helps the model learn more complex patterns.\r\n",
    "- Bias is added to every neuron to improve model flexibility, performance, and generalization.\r\n",
    "- It ensures that neurons can activate even with zero inputs, contributing to effective learning and optimization.\r\n",
    "\r\n",
    "Bias is a fundamental concept in neural networks that enhances the capability of the model to learn and adapt to diverse data patterns.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e24ec-c8cd-4e88-8c0b-8291d9b13280",
   "metadata": {},
   "source": [
    "What is Bias in Neural Networks?\n",
    "1. Definition\n",
    "In the context of neural networks, the bias is an additional parameter added to the weighted sum of inputs before applying the activation function. Mathematically, if \n",
    "ùëã\n",
    "X is the input vector, \n",
    "ùëä\n",
    "W is the weight vector, and \n",
    "ùëè\n",
    "b is the bias, the output of a neuron can be expressed as:\n",
    "\n",
    "ùë¶\n",
    "=\n",
    "activation\n",
    "(\n",
    "ùëä\n",
    "‚ãÖ\n",
    "ùëã\n",
    "+\n",
    "ùëè\n",
    ")\n",
    "y=activation(W‚ãÖX+b)\n",
    "where:\n",
    "\n",
    "ùëä\n",
    "‚ãÖ\n",
    "ùëã\n",
    "W‚ãÖX represents the dot product of the weights and inputs,\n",
    "ùëè\n",
    "b is the bias term,\n",
    "activation\n",
    "activation is the activation function applied to the weighted sum plus bias.\n",
    "2. Purpose of Bias\n",
    "Shifting the Activation Function: The bias allows the activation function to be shifted to the left or right, which enables the model to fit the data better. Without the bias, the activation function is always centered at zero, which might not be suitable for all datasets.\n",
    "\n",
    "Enabling Learning of Complex Patterns: Bias helps the network learn patterns and relationships in the data that might not be possible if the activation function were always centered at zero. It provides more flexibility to the model by allowing it to fit a broader range of functions.\n",
    "\n",
    "Handling Zero Inputs: Bias ensures that neurons can activate even if all the input values are zero. This is particularly important because it means the neuron can still produce a non-zero output, which helps in learning and optimization.\n",
    "\n",
    "3. Why is Bias Added Every Time?\n",
    "Generalization: Bias is added to every neuron to ensure that each neuron can independently adjust its output regardless of the input values. This individual adjustment capability helps in better generalization of the model to various types of data.\n",
    "\n",
    "Improving Model Performance: Including bias improves the overall performance of the neural network by enabling it to learn a wider variety of functions. This flexibility often results in better fitting of the training data and potentially better generalization to unseen data.\n",
    "\n",
    "Simplicity and Consistency: Adding a bias term is a standard practice in neural networks and contributes to the simplicity and consistency of the model architecture. It is a well-established technique that helps in optimizing and training neural networks effectively.\n",
    "\n",
    "4. Example\n",
    "Consider a simple neuron with a single input feature. If we denote the weight as \n",
    "ùë§\n",
    "w, the input as \n",
    "ùë•\n",
    "x, and the bias as \n",
    "ùëè\n",
    "b, the output before applying the activation function can be computed as:\n",
    "\n",
    "ùëß\n",
    "=\n",
    "ùë§\n",
    "‚ãÖ\n",
    "ùë•\n",
    "+\n",
    "ùëè\n",
    "z=w‚ãÖx+b\n",
    "Here, the bias \n",
    "ùëè\n",
    "b adjusts the value of \n",
    "ùëß\n",
    "z, allowing the activation function to output values that are not restricted by the zero-centered nature of the weighted sum alone.\n",
    "\n",
    "5. Impact on Learning\n",
    "During the training process of a neural network, both weights and biases are adjusted based on the gradients computed during backpropagation. This adjustment helps minimize the loss function and improves the accuracy of the model. Biases play a critical role in this adjustment by allowing each neuron to adapt its output independently of the input values.\n",
    "\n",
    "Summary\n",
    "Bias is an additional parameter added to the weighted sum of inputs in a neural network.\n",
    "It allows the activation function to be shifted, which helps the model learn more complex patterns.\n",
    "Bias is added to every neuron to improve model flexibility, performance, and generalization.\n",
    "It ensures that neurons can activate even with zero inputs, contributing to effective learning and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749f56d-a190-44ab-bdd4-08fb55279daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
